"""
"""

import logging
import mesos
import os
import progressbar
import StringIO
import tarfile
import tempfile
import threading
import uuid

from fs.osfs import OSFS
from fs.s3fs import S3FS

from Queue import Queue
from urlparse import urlparse
from ddocker import subcommand
from ddocker.parser import parse_dockerfile
from ddocker.proto import ddocker_pb2
from ddocker.proto import mesos_pb2

logger = logging.getLogger("ddocker.build")


def args(parser):
    parser.add_argument("dockerfile")

    # Arguments for the staging filesystem
    group = parser.add_argument_group("fs")
    group.add_argument("--staging-uri", default="/tmp/ddocker",
                       help="The URI to use as a base directory for staging files.")
    group.add_argument("--s3-access-key", default=os.environ.get("AWS_ACCESS_KEY_ID"),
                       help="Access key for using the S3 filesystem")
    group.add_argument("--s3-secret-key", default=os.environ.get("AWS_SECRET_ACCESS_KEY"),
                       help="Secret key for using the S3 filesystem")


@subcommand("build", callback=args)
def main(args):

    logger.info("Building docker image from %s", args.dockerfile)

    task_queue = Queue()

    # Launch the mesos framework
    framework = mesos_pb2.FrameworkInfo()
    framework.user = ""  # Mesos can select the user
    framework.name = "ddocker"
    framework.failover_timeout = 300  # Timeout after 300 seconds

    # Figure out the framework info from an existing checkpoint
    if args.checkpoint and os.path.exists(args.checkpoint):
        with open(args.checkpoint) as cp:
            checkpoint = ddocker_pb2.Checkpoint()
            ddocker_pb2.Checkpoint.ParseFromString(
                checkpoint, cp.read()
            )

            if not checkpoint.frameworkId:
                raise RuntimeError("No framework ID in the checkpoint")

            logger.info("Registering with checkpoint framework ID %s", checkpoint.frameworkId)
            framework.id.value = checkpoint.frameworkId

    # Kick off the scheduler driver
    scheduler = Scheduler(task_queue, args.checkpoint)
    driver = mesos.MesosSchedulerDriver(
        scheduler, framework, args.mesos_master
    )

    thread = threading.Thread(target=driver.run)
    thread.setDaemon(True)
    thread.start()

    # Parse the dockerfile
    dockerfile = parse_dockerfile(args.dockerfile)

    # Generate the dockerfile build context
    _, context_path = tempfile.mkstemp()
    context = open(context_path, "w+b")

    logger.debug("Writing context tar to %s", context_path)

    context_root = os.path.abspath(os.path.dirname(args.dockerfile))
    context_size = make_build_context(context, context_root, dockerfile)

    # Upload the build context to the staging filesystem
    filesystem = create_filesystem(
        staging_uri=args.staging_uri,
        s3_key=args.s3_access_key,
        s3_secret=args.s3_secret_key
    )

    staging_file = "%s.tar.gz" % uuid.uuid1()
    staging_uri = os.path.join(args.staging_uri, staging_file)

    logger.info("Uploading context to %s (%r bytes)", staging_uri, context_size)

    # Create a progress bar
    pbar = progressbar.ProgressBar(maxval=context_size)
    event = filesystem.setcontents_async(
        path=staging_file,
        data=context,
        progress_callback=pbar.update,
        finished_callback=pbar.finish
    )

    # Wait for the file to upload...
    event.wait()

    # Close and clear up the tmp context
    logger.info("Cleaning up local context %s", context_path)
    context.close()
    os.unlink(context_path)

    # Create a mesos task and ship it to the framework
        # Download the tar to the sandbox
        # POST /build (stdin = the tar)
        # Tag and push the image
    # Return and BAM. DONE.

    # thread.join(20)


def make_build_context(output, context_root, dockerfile):
    """Generate and return a compressed tar archive of the build context."""

    tar = tarfile.open(mode="w:gz", fileobj=output)
    for idx, (cmd, instruction) in enumerate(dockerfile.instructions):
        if cmd == "ADD":
            local_path, remote_path = instruction
            tar_path = str(idx)

            if not local_path.startswith("/"):
                local_path = os.path.join(context_root, local_path)
            local_path = os.path.abspath(local_path)

            logger.debug("Adding path %s to tar at %s", local_path, tar_path)
            tar.add(local_path, arcname=tar_path)
            dockerfile.instructions[idx] = (cmd, (tar_path, remote_path))

    # Write the modified dockerfile into the tar also
    buildfile = StringIO.StringIO()
    buildfile.write("# Generated by ddocker\n")

    for cmd, instructions in dockerfile.instructions:
        line = "%s %s" % (cmd, " ".join(instructions))

        logger.debug("Added command %r to new dockerfile", line)
        buildfile.write("%s\n" % line)

    buildfile.seek(0, os.SEEK_END)
    info = tarfile.TarInfo("Dockerfile")
    info.size = buildfile.tell()

    buildfile.seek(0)
    tar.addfile(info, fileobj=buildfile)

    tar.close()
    output.seek(0, os.SEEK_END)
    tar_size = output.tell()
    output.seek(0)

    return tar_size


def create_filesystem(staging_uri, s3_key, s3_secret):
    """Create an instance of a filesystem based on the URI"""

    url = urlparse(staging_uri)

    # Local filesystem
    if not url.scheme:
        return OSFS(
            root_path=url.path,
            create=True
        )

    # S3 filesystem
    if url.scheme.lower() == "s3":
        if not url.netloc:
            raise Exception("You must specify a s3://bucket/ when using s3")
        return S3FS(
            bucket=url.netloc,
            prefix=url.path,
            aws_access_key=s3_key,
            aws_secret_key=s3_secret,
            key_sync_timeout=3
        )


class Scheduler(mesos.Scheduler):

    def __init__(self, task_queue, checkpoint):
        self.task_queue = task_queue
        self.checkpoint_path = checkpoint

    def registered(self, driver, frameworkId, masterInfo):
        logger.info("Framework registered with %s", frameworkId.value)
        self._checkpoint(frameworkId=frameworkId.value)

    def _checkpoint(self, frameworkId=None):
        """Helper method for persisting checkpoint information."""

        if not self.checkpoint_path:
            logger.debug("Skipping checkpoint, not enabled")
            return

        exists = os.path.exists(self.checkpoint_path)
        with open(self.checkpoint_path, "w+b") as cp:
            checkpoint = ddocker_pb2.Checkpoint()
            if exists:
                ddocker_pb2.Checkpoint.ParseFromString(checkpoint, cp.read())
                cp.seek(0)

            if frameworkId:
                checkpoint.frameworkId = frameworkId

            logger.debug("Written checkpoint to %s", self.checkpoint_path)

            cp.write(checkpoint.SerializeToString())
            cp.truncate()
